# Omnivorous modeling for visual modalities

We adapt the code base for OmniMAE from https://github.com/facebookresearch/omnivore.

We implement the two-stage action recognition pretraining method: Privacy-Preserving MAE Align method. 
This code base trains and evaluates on the benchmark we propose.

Refer to ```omnivision/README.md``` for instructions on running the code base.



## License
This code base is released under the CC-BY-NC 4.0 license. See [LICENSE](LICENSE) for additional details. 